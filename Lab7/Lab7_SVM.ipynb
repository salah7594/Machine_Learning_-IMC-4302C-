{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 7: Support Vector Machine (SVM)\n",
    "\n",
    "In this session, we will study some aspects of the SVM classifier and regressor. The training and optimization processes of the SVM models are out of the scope of this labs because it includes some complex mathematical equations and algorithm. Therefore, we will use the [SVM](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm) module from sklearn library.\n",
    "\n",
    "First, we will compare the logistic regression and the SVM classifiers on a simple and separable dataset. Then, we will introduce the kernel trick that enhance capabilities of the SVM. We will work with \"Gaussian\" kernel and try to tune the model meta-parameters like the regularization parameter $\\lambda$ and the standard deviation $\\sigma$ of the \"Gaussian\" kernel. Finally, we will use the SVM for a regression problem and we will see how it works.\n",
    "\n",
    "### SVM vs Logistic Classifiers\n",
    "The SVM classifier use a specific sample called \"_support vector_\" to define an hyperplane that divides the space into two regions: one for positive and one for negative labels. The equation of the hyperplane is defined by: $\\theta^\\top x=\\theta_0+\\theta_1 x_1+\\dots+\\theta_{n-1} x_{n-1}=0$. The SVM classifier try to maximize the margin between hyperplane and \"_support vectors_\" (set of closest samples to the chosen hyperplane) \n",
    "\n",
    "Note that the regularized cost function of a logistic classifiers is given by the following equation:\n",
    "\n",
    "$$J_{Reg}(\\theta)= \\frac{1}{m}\\sum_{i=1}^{m}\\left [y_i\\times (-log(sigmoid(\\theta^\\top x_i)))+(1-y_i)\\times (-log(1-sigmoid(\\theta^\\top x_i)))\\right ]+\\frac{\\lambda}{2m}\\times\\sum_{j=1}^{n-1} \\theta_j^2$$\n",
    "\n",
    "The cost function used in the SVM classifier is:\n",
    "\n",
    "$$J_{SVM}(\\theta)= C\\sum_{i=1}^{m}\\left [y_i\\times cost_1(\\theta^\\top x_i)+(1-y_i)\\times cost_0(\\theta^\\top x_i)\\right ]+\\frac{1}{2}\\times\\sum_{j=1}^{n-1} \\theta_j^2 $$\n",
    "$cost_0$ and $cost_1$ are two functions that penalize a sample \"i\" if it is not on the right side of the hyperplane. We notice that the SVM regularization parameter $C$ is the inverse of the logistic regularization parameter $\\lambda$. Thus we write: $C=\\frac{1}{\\lambda}$  \n",
    "\n",
    "<font color=\"blue\">**Question 1: **</font> The \"data_1.txt\" file contains 3 columns: 2 for the features \"x_1\" and \"x_2\" and the last column is for the labels \"y\".\n",
    "- Load data from \"data_1.txt\" file. (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library).\n",
    "- Extract \"x_1\", \"x_2\" and \"y\" variables from \"data\" columns. (don't forget to add new axis to have 2D array with the right shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fmin_bfgs\n",
    "\n",
    "# load and extract data\n",
    "data=     # ** your code here** \n",
    "print(\"the size of data_1 is:\",data.shape)\n",
    "\n",
    "m = data.shape[0] # number of samples\n",
    "x_1 =   # ** your code here**\n",
    "x_2 =   # ** your code here**\n",
    "y =     # ** your code here**\n",
    "\n",
    "X=np.concatenate((np.ones((m,1)),x_1,x_2),axis=1)  \n",
    "n=X.shape[1] # number of features\n",
    "\n",
    "# regularized cost and gradient function\n",
    "def sigmoid(z):\n",
    "    return np.ones(z.shape)/(1+np.exp(-z))\n",
    "def Reg_cost_func(theta):\n",
    "    J=np.sum(-y*np.log(sigmoid(np.dot(X,theta[:,np.newaxis]))))-np.sum((1-y)*np.log(1-sigmoid(np.dot(X,theta[:,np.newaxis]))))+lambda_/2*np.sum(theta[1:]**2)\n",
    "    return J/m  \n",
    "def Reg_grad_cost_func(theta):\n",
    "    g=(1/m)*(np.dot(X.transpose(),(sigmoid(np.dot(X,theta[:,np.newaxis]))-y))+lambda_*np.concatenate((np.zeros((1,1)),theta[1:,np.newaxis]),axis=0))  # this is the vectorized implementation\n",
    "    g.shape=(g.shape[0],)\n",
    "    return g  \n",
    "\n",
    "# regularization parameter\n",
    "lambda_=1\n",
    "\n",
    "# calculate theta_opt\n",
    "theta0=np.zeros((n,),dtype=float)\n",
    "theta_opt= fmin_bfgs(Reg_cost_func,theta0,fprime=Reg_grad_cost_func,disp=0)\n",
    "print(\"the optimal vector theta is:\", theta_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 2: **</font> Now you should use the SVM classifier with a linear kernel on the same \"data_1\" and compare the result with the logistic classifier.\n",
    "- Define the SVM regularization parameter \"C\" from the previous assigned value to \"lambda\". (Note: $C=\\frac{1}{\\lambda}$. Be careful to the case $\\lambda=0$) \n",
    "- Use [SVM classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) class from  sklearn library to define \"lin_svm\" a SVM classifier with a regularization parameter \"C\" and a \"linear\" kernel. \n",
    "- Use [fit](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit) function of \"lin_svm\" object to train the model. Then, compare the result of the two classifiers (Logistic and SVM)\n",
    "- Try different value of regularization parameter $\\lambda$ ( or C parameter) like 0, 1, 100.. what do you notice in case there is no regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# format data for SVM cassifier\n",
    "X2=np.concatenate((x_1,x_2),axis=1) \n",
    "y2=y[:,0]\n",
    "\n",
    "# create and train SVM classifier\n",
    "C= # ** your code here**\n",
    "lin_svm =  # ** your code here** \n",
    "    # ** your code here**\n",
    "\n",
    "#compare two classifier result\n",
    "print(\"the intercept term for logistic classifier is:\", theta_opt[0], \"while for SVM classifier it is: \",lin_svm.intercept_)\n",
    "print(\"the features 'x_1' and 'x_2' coefficients for logistic classifier are:\", theta_opt[1:], \"while for SVM classifier they are: \",lin_svm.coef_[0,:])\n",
    "\n",
    "# plot data and models\n",
    "x_1_min = np.min(X[:,1])\n",
    "x_1_max = np.max(X[:,1])\n",
    "plt.figure(\"Visualize data_1 and models\",figsize=(9,5))\n",
    "plt.scatter(x_1[y==0], x_2[y==0],  color='red',label='fail')\n",
    "plt.scatter(x_1[y==1], x_2[y==1],  color='green',marker='+',s=80, label='success')\n",
    "plt.plot([x_1_min,x_1_max],[(-theta_opt[0]-theta_opt[1]*x_1_min)/theta_opt[2],(-theta_opt[0]-theta_opt[1]*x_1_max)/theta_opt[2]],label=\"logistic classifier\")\n",
    "plt.plot([x_1_min,x_1_max],[(-lin_svm.intercept_[0]-lin_svm.coef_[0,0]*x_1_min)/lin_svm.coef_[0,1],(-lin_svm.intercept_[0]-lin_svm.coef_[0,0]*x_1_max)/lin_svm.coef_[0,1]],label=\"SVM classifier\")\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "legend = plt.legend(loc='best', shadow=True, fontsize='x-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM kernels trick for more complex data \n",
    "If the data is complex and the linear decision boundary is not sufficient to separate it, we could apply a transformation on our data and project it to a new space using kernels. Then, we could use the SVM classifier on this new space with a higher dimension (up to infinite dimension) generally where it is more likely to find an hyperplane that separates well the data. Hence, the hyperplane equation becomes: $h_\\theta(x)=\\theta^\\top \\phi(x)+\\theta_0=0$.  \n",
    "\n",
    "In fact, the transformation $\\phi$ is a complex function and in most cases, it is not known explicitly. However, the SVM optimization process allows us to calculate $\\alpha^*_i$'s where: \n",
    "$ \\left\\{\\begin{matrix}\n",
    "h_\\theta(x)=\\sum_{i=1}^m\\alpha^*_i K(x,x_i)+\\theta_0\n",
    "\\\\ K(x,x_i)=\\phi(x)^\\top\\phi(x_i)\n",
    "\\end{matrix}\\right.$  \n",
    "\n",
    "$K(x,x_i)$ represents the kernel and its formulas is known explicitly in contrast to $\\phi(x)$. In this part, we will use the \"Gaussian\" kernel (a.k.a. \"Radial Basis Function\")  given by: $$K(x_1,x_2)=exp(\\frac{-\\left \\| x_1-x_2 \\right \\|^2}{2\\sigma^2})=exp(-\\gamma \\left \\| x_1-x_2 \\right \\|^2)$$\n",
    "\n",
    "<font color=\"blue\">**Question 3: **</font> The \"data_2.txt\" file contains 3 columns: 2 for the features \"x_1\" and \"x_2\" and the last column is for the labels \"y\".\n",
    "- Load data from \"data_2.txt\" file. (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library).\n",
    "- Extract \"x_1\", \"x_2\" and \"y\" variables from \"data\" columns. (don't forget to add new axis to have 2D array with the right shape) \n",
    "- Implement \"Gaussian_kernel\" function according to the given equation above.\n",
    "- Use [SVM classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) class from  sklearn library to define \"svm_clf\" a SVM classifier with a regularization parameter \"C\" and a \"precomputed\" kernel. \n",
    "- Use [fit](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit) function of \"svm_clf\" object and \"X\" and \"y\" vectors to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# load and extract data\n",
    "data =    # ** your code here** \n",
    "print(\"the size of data_2 is:\",data.shape)\n",
    "\n",
    "m = data.shape[0] # number of samples\n",
    "x_1 =   # ** your code here**\n",
    "x_2 =   # ** your code here**\n",
    "y =     # ** your code here**\n",
    "\n",
    "# Gaussian Kernel\n",
    "sigma=0.1     # standard deviation\n",
    "\n",
    "def Gaussian_kernel(x1,x2,sigm):\n",
    "    K =     # ** your code here**\n",
    "    return K\n",
    "\n",
    "# compute transformed data\n",
    "X10,X11=np.meshgrid(data[:,0], data[:,0])\n",
    "X20,X21=np.meshgrid(data[:,1], data[:,1])\n",
    "X0=np.concatenate((X10[...,np.newaxis],X20[...,np.newaxis]),axis=-1)  \n",
    "X1=np.concatenate((X11[...,np.newaxis],X21[...,np.newaxis]),axis=-1) \n",
    "X=Gaussian_kernel(X0,X1,sigma)\n",
    "\n",
    "# SVM classifier\n",
    "C=1\n",
    "svm_clf =   # ** your code here**\n",
    "   # ** your code here**\n",
    "\n",
    "# calculate the mesh grid for decision boundary\n",
    "u1=np.linspace(0,1,100)\n",
    "u2=np.linspace(0.4,1,100)\n",
    "u1, u2 = np.meshgrid(u1, u2)\n",
    "XX0=np.concatenate((np.tile(u1[...,np.newaxis,np.newaxis],(1,1,m,1)),np.tile(u2[...,np.newaxis,np.newaxis],(1,1,m,1))),axis=-1)\n",
    "XX1=np.concatenate((np.tile(data[np.newaxis,np.newaxis,:,0,np.newaxis],(*u1.shape,1,1)),np.tile(data[np.newaxis,np.newaxis,:,1,np.newaxis],(*u1.shape,1,1))),axis=-1)\n",
    "X3=Gaussian_kernel(XX0,XX1,sigma)\n",
    "X3bis=X3.reshape((X3.shape[0]*X3.shape[1],X3.shape[2]))\n",
    "Zbis =  svm_clf.predict(X3bis)\n",
    "Z=Zbis.reshape((X3.shape[0],X3.shape[1]))\n",
    "\n",
    "# plot data and decision boundary\n",
    "plt.figure(\"Visualize data_2 and models\",figsize=(9,5))\n",
    "fail=plt.scatter(x_1[y==0], x_2[y==0],  color='red',label='fail')\n",
    "succ=plt.scatter(x_1[y==1], x_2[y==1],  color='green',marker='+',s=80, label='success')\n",
    "ctr = plt.contour(u1, u2, Z,1,colors=\"blue\",alpha=.7)\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "precom_bound = Rectangle((0, 0), 3, 4, fc=\"w\", fill=False, edgecolor=\"b\", linewidth=1)\n",
    "plt.legend([precom_bound,fail,succ], (\"boundary (precomputed SVM)\",\"fail\",\"success\"),loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 4: **</font> Now we will use the SVM classifier with the \"rbf\" (Gaussian) kernel that computes transformed data automatically instead of computing it using the implemented \"Gaussian_kernel\" function. \n",
    "- Define the Gaussian kernel parameter \"Gamma\" from the previously assigned value to the standard deviation \"sigma\". (Recall: $\\gamma=\\frac{1}{2\\sigma^2}$).\n",
    "- Use the [SVM classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) class from  sklearn library to define \"gauss_svm\" an SVM classifier with a regularization parameter \"C\", an \"rbf\" kernel and \"Gamma\" parameter.\n",
    "- Use [fit](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit) function of \"gauss_svm\" object and \"X\" and \"y\" vectors to train the model. Then, compare result with the previous classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.concatenate((x_1,x_2),axis=1)\n",
    "\n",
    "# SVM classifier\n",
    "Gamma =   # ** your code here**\n",
    "gauss_svm = # ** your code here**\n",
    "  # ** your code here**\n",
    "\n",
    "# calculate the mesh grid for decision boundary\n",
    "u1=np.linspace(0,1,100)\n",
    "u2=np.linspace(0.4,1,100)\n",
    "u1, u2 = np.meshgrid(u1, u2)\n",
    "X33=np.concatenate((u1[...,np.newaxis],u2[...,np.newaxis]),axis=-1)\n",
    "X33bis=X33.reshape((X33.shape[0]*X33.shape[1],X33.shape[2]))\n",
    "Zbis =  gauss_svm.predict(X33bis)\n",
    "Z3=Zbis.reshape((X33.shape[0],X33.shape[1]))\n",
    "\n",
    "#plot decision boundary\n",
    "ctr = plt.contour(u1, u2, Z3,1,colors=\"magenta\",alpha=.7)\n",
    "gauss_bound = Rectangle((0, 0), 3, 4, fc=\"w\", fill=False, edgecolor=\"m\", linewidth=1)\n",
    "plt.legend([precom_bound,gauss_bound,fail,succ], (\"boundary (precomputed SVM)\",\"boundary (gaussian SVM)\",\"fail\",\"success\"),loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune standard deviation $\\sigma$ with K-fold cross validation\n",
    "\n",
    "In this part, we will tune the standard deviation $\\sigma$ of the Gaussian kernel using K-fold method to evaluate the performance on cross validation set. Then, we will select the value of sigma that gives the best accuracy.\n",
    "The K-fold consists dividing the training set to K blocks. Then, we consider one of the blocks as validation set (or test set)  and we consider the rest of data as training set. After fitting the model we evaluate its performance en the chosen validation fold (group). We repeat this process on all the K-folds. Hence, we get K estimations of the performance then we calculate their mean to get a more reliable estimation.\n",
    "![k-fold-diagram](k-fold-diagram.png)\n",
    "\n",
    "<font color=\"blue\">**Question 5: **</font> \n",
    "- Load the data from \"data_3.txt\" file. (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library).\n",
    "- Extract the cross validation set (\"X_val\" and \"y_val\") from \"X\" and \"y\" data.\n",
    "- Use [fit](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR.fit) function of \"gauss_svm\" object and \"X_train\" and \"y_train\" vectors to train the model. \n",
    "- Use [predict](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR.predict) function of \"gauss_svm\" object and predict \"y_val_pred\" the output of cross validation data \"X_val\". \n",
    "- Calculate the mean of \"val_accuracy\" vector that represents the accuracy among all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "# load data\n",
    "data =    # ** your code here** \n",
    "\n",
    "# shuffle data\n",
    "np.random.seed(5190969)\n",
    "data=np.random.permutation(data)\n",
    "print(\"the size of data_3 is:\",data.shape)\n",
    "\n",
    "#extract data\n",
    "m = data.shape[0] # number of samples\n",
    "x_1=data[:,0,np.newaxis]  \n",
    "x_2=data[:,1,np.newaxis]  \n",
    "y=data[:,2]    \n",
    "print(y.sum())\n",
    "X=np.concatenate((x_1,x_2),axis=1)  \n",
    "\n",
    "\n",
    "C = 1  # regularization parameter\n",
    "K = 5  # number of cross validation folds\n",
    "fold_size = int(floor(m/K))  #size of fold\n",
    "sigma_list = [0.01, 0.03, 0.1,0.3, 1,3]\n",
    "\n",
    "# loop over different sigma values\n",
    "for sigma in sigma_list:\n",
    "    Gamma=1/2/(sigma**2) \n",
    "    val_accuracy=np.zeros((K,))\n",
    "    # loop over different folds and calculate the mean accuracy\n",
    "    for k in range(K):\n",
    "        X_val =   # ** your code here**\n",
    "        y_val =   # ** your code here**\n",
    "        X_train = np.delete(X,np.s_[k*fold_size:(k+1)*fold_size],axis=0)\n",
    "        y_train = np.delete(y,np.s_[k*fold_size:(k+1)*fold_size],axis=0)\n",
    "        gauss_svm = svm.SVC(C=C,kernel='rbf',gamma=Gamma)\n",
    "        # ** your code here**\n",
    "        y_val_pred =  # ** your code here**\n",
    "        val_accuracy[k]=(y_val==y_val_pred).sum()/fold_size*100\n",
    "        \n",
    "    mean_val_accuracy =  # ** your code here**\n",
    "    print(\"The {}-fold cross validation mean accuracy for \\u03C3=\".format(K), sigma,\" is: \",mean_val_accuracy,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 6: **</font> \n",
    "- Select \"best_sigma\" value that gives a maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train best svm classifier\n",
    "best_sigma =  # ** your code here**\n",
    "gauss_svm = svm.SVC(C=C,kernel='rbf',gamma=1/2/(best_sigma**2) )\n",
    "gauss_svm.fit(X_train, y_train)\n",
    "\n",
    "# calculate the mesh grid for contour plot\n",
    "u1=np.linspace(-0.6,0.3,500)\n",
    "u2=np.linspace(-0.8,0.6,500)\n",
    "u1, u2 = np.meshgrid(u1, u2)\n",
    "X33=np.concatenate((u1[...,np.newaxis],u2[...,np.newaxis]),axis=-1)\n",
    "X33bis=X33.reshape((X33.shape[0]*X33.shape[1],X33.shape[2]))\n",
    "Zbis =  gauss_svm.predict(X33bis)\n",
    "Z3=Zbis.reshape((X33.shape[0],X33.shape[1]))\n",
    "\n",
    "# plot data and boundary\n",
    "plt.figure(\"Visualize data_3 and boundary\",figsize=(9,5))\n",
    "fail=plt.scatter(x_1[y==0], x_2[y==0],  color='red',label='fail')\n",
    "succ=plt.scatter(x_1[y==1], x_2[y==1],  color='green',marker='+',s=80, label='success')\n",
    "ctr = plt.contour(u1, u2, Z3,1,colors=\"blue\")\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "plt.legend([precom_bound,fail,succ], (\"decision boundary\",\"fail\",\"success\"),loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine for regression\n",
    "\n",
    "There is an SVM version for a regression problem. The idea is to calculate a hypothesis function using the following form: $h_\\theta(x)=\\theta^\\top \\phi(x)+\\theta_0$. Then, it tries to minimize a cost function that penalizes only the points that are far from the calculated model ($h_\\theta(x)$) more than $\\epsilon$.\n",
    "![Support Vector Regression](SVR.png)\n",
    "\n",
    "The SVR cost function is given by:\n",
    "$$J_{SVR}(\\theta)=\n",
    "\\left\\{\\begin{matrix}\n",
    " 0& if \\left | y-h_\\theta(x)) \\right |\\leq\\epsilon\\\\ \n",
    "\\left | y-h_\\theta(x)) \\right |-\\epsilon  & otherwise \n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "In this part, we will use SVR (Support Vector Regression) with \"rbf\" (Gaussian) kernel and with \"epsilon\" parameter in order to optimize the previous cost function and determine a regression model for the data.\n",
    "<font color=\"blue\">**Question 7: **</font> \n",
    "- Use [SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) class from  sklearn library to define \"svm_regr\" a SVM classifier with Regularization parameter \"C=1\", \"rbf\" kernel, \"gamma\"=0.5 and \"epsilon\"=0.1.\n",
    "- Use [fit](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR.fit) function of \"svm_regr\" object and \"X\" and \"y\" vectors to train the model. \n",
    "- Use [predict](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR.predict) function of \"svm_regr\" object and predict \"y_pred\" the output of data \"X\". \n",
    "- Try to train the model with different values of the parameter \"epsilon\" of SVR (try values like 0.1, 0.5, 1,1.5). What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics.regression import mean_squared_error\n",
    "\n",
    "# generate random data\n",
    "np.random.seed(0)\n",
    "m = 15\n",
    "x = np.linspace(0,10,m) + np.random.randn(m)/5\n",
    "y = np.sin(x)+x/6 + np.random.randn(m)/10\n",
    "X=x[:,np.newaxis]\n",
    "\n",
    "# SVM regression model\n",
    "svm_regr =   # ** your code here**\n",
    "# ** your code here**\n",
    "y_pred =    # ** your code here**\n",
    "\n",
    "print(\"The mean squared error of the SVR model is:\",mean_squared_error(y, y_pred))\n",
    "\n",
    "# calculate and plot the model\n",
    "X1=np.linspace(0,10,100)\n",
    "Y =  svm_regr.predict(X1[:,np.newaxis])\n",
    "plt.figure(\"different polynomial models\",figsize=(9,5))\n",
    "plt.plot(X, y, 'o', label='training data', markersize=10)\n",
    "plt.plot(X1, Y,label=\"SVM regression model\")\n",
    "plt.legend(loc='best', shadow=True, fontsize='x-large')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
