{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Principal Component Analysis (PCA)\n",
    "\n",
    "In this session, we will apply the principal component analysis (PCA) technique to reduce the dimension of some data. This reduction allows us to decrease the number of features with conserving most of the information in the data which accelerates the learning algorithm without affecting a lot its expressiveness. In addition, the PCA helps to compress data and recover it without loosing a lot of significant information. Besides, in some cases, this technique helps to visualize on 2D and 3D graphs higher dimension data not visualizable in the original dimension.\n",
    "\n",
    "First, we will implement the PCA on 2D data and visualize the original and recovered data. Then, we will calculate the error between recovered data and original one. In the second part, we will generate low rank 3D data and we will use [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) module from sklearn library to project it in a 2D plane. Then, we will try to visualize the data projected in 2D plane. Finally, we will work with \"faces\" dataset and we will try to compress it using PCA. Then, we will calculate the compression rate, the error and see how to choose the reduction dimension.\n",
    "\n",
    "### Implement PCA\n",
    "In this part, we will use Singular Vale Decomposition (SVD) to implement PCA. In fact ,the SVD determines the eigenvalues  and their corresponding  eigenvectors in decreasing order. the first K eigen vectors represent the K most important components of our data. Hence, we will project our data on these eigenvectors. Then, we will recover the data by doing the inverse transformation and we will calculate the error.\n",
    "\n",
    "<font color=\"blue\">**Question 1: **</font> \n",
    "- Load the data from \"2D_data.txt\" file in \"X\" variable and check its size. (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library) \n",
    "- Calculate the mean \"mu\" and the standard deviation \"std\" of the data. Then, calculate \"X_norm\" the normalized data. \n",
    "- Calculate the covariance matrix \"SIGMA\" given by: $\\Sigma=\\frac{1}{m}X_{norm}^\\top X_{norm}$.\n",
    "- Use [svd](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) function from sklearn library to make singular value decomposition. This function will compute 3 arrays: U, S and V. The arrays \"U\" and \"V\" contain the eigenvectors. They are orthogonal matrix so the inverse is equal to the transpose and they are the inverses of each other. While \"S\" contains the eigenvalues. These arrays verify the following formulas:$$\\Sigma=U\\times diag(S)\\times V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# load data\n",
    "X =   # ** your code here** \n",
    "print(\"The size of data is:\",X.shape)\n",
    "m = X.shape[0]\n",
    "\n",
    "K=1 # reduction dimension\n",
    "\n",
    "# normalize data\n",
    "mu =    # ** your code here** \n",
    "std =    # ** your code here** \n",
    "X_norm =   # ** your code here** \n",
    "\n",
    "# Singular value decomposition\n",
    "SIGMA =  # ** your code here** \n",
    "print(\"The size of SIGMA is:\",SIGMA.shape)\n",
    "U,S,V =  # ** your code here** \n",
    "print(\"The size of U is:\",U.shape)\n",
    "print(\"The size of S is:\",S.shape)\n",
    "print(\"The size of V is:\",V.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 2: **</font> \n",
    "- Calculate the projected data \"Z\" given by: $Z=X_{norm}\\times U_K$  \n",
    "Where: $U_K$ represent the first K columns of the $U$ matrix.\n",
    "- Calculate the recovered data given by: $X_{rec}=Z\\times U_K^{-1}=Z\\times U_K^\\top$ \n",
    "- Calculate the projection error: $\\frac{1}{m}\\sum_{i=1}^m\\left \\| x_{norm}^{(i)}-x_{rec}^{(i)} \\right \\|^2$  \n",
    "where: $\\left \\| x \\right \\|^2=\\sum_{j=1}^nx_j^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project dataon K dimension\n",
    "Z =   # ** your code here** \n",
    "\n",
    "# recover data\n",
    "X_rec =  # ** your code here** \n",
    "\n",
    "# calculate error \n",
    "proj_error =  # ** your code here** \n",
    "print(\"The projection error is:\",proj_error)\n",
    "\n",
    "# plot data\n",
    "fig = plt.figure(\"PCA 2D data\",figsize=(9,4.5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.scatter(X[:,0],X[:,1],color=\"blue\",marker='o',facecolors='none')\n",
    "ax.quiver(mu[0],mu[1],S[0]*U[0,0],S[0]*U[1,0],color='green',scale=0.75,scale_units='xy',angles='xy')\n",
    "ax.quiver(mu[0],mu[1],S[1]*U[0,1],S[1]*U[1,1],color='green',scale=0.75,scale_units='xy',angles='xy')\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_xlim(0.8,0.8+5.5)\n",
    "ax.set_ylim(2.2,2.2+5.5)\n",
    "ax.set_title(\"Principal Components (projection direction)\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.scatter(X[:,0],X[:,1],color=\"blue\",marker='o',facecolors='none')\n",
    "ax.scatter(std[0]*X_rec[:,0]+mu[0],std[1]*X_rec[:,1]+mu[1],color='green',marker='o',facecolors='none')\n",
    "for i in range(m):\n",
    "    ax.plot([X[i,0],std[0]*X_rec[i,0]+mu[0]],[X[i,1],std[1]*X_rec[i,1]+mu[1]],'r--',linewidth=1)\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_xlim(0.8,0.8+5.5)\n",
    "ax.set_ylim(2.2,2.2+5.5)\n",
    "ax.set_title(\"Projection error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with Sklearn\n",
    "In this part, we will use sklearn library functions to generate low rank 3D data and apply the PCA on it. Then, we will visualize it in a 2D plane.\n",
    "\n",
    "<font color=\"blue\">**Question 3: **</font> \n",
    "- Generate low rank data of size $50\\times 3$. You should use [make_low_rank_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_low_rank_matrix.html#sklearn.datasets.make_low_rank_matrix) function with the following parameters: \"effective_rank=2\",\"tail_strength=0.01\" and \"random_state=0\"\n",
    "- Calculate the mean \"mu\" and the standard deviation \"std\" of the data. Then, calculate \"X_norm\" the normalized data. \n",
    "- Use [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) module from sklearn library to create \"pca\" object with \"n_components\" equal to \"K\".\n",
    "- Call [fit](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit) function of \"pca\" object with normalized data \"X_norm\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from mpl_toolkits.mplot3d.axes3d import*\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# generate data\n",
    "X =   # ** your code here**\n",
    "print(\"The size of data is:\",X.shape)\n",
    "m = X.shape[0]\n",
    "\n",
    "K = 2 # reduction dimension\n",
    "\n",
    "# normalize data\n",
    "mu =   # ** your code here** \n",
    "std =    # ** your code here** \n",
    "X_norm =  # ** your code here** \n",
    "\n",
    "# PCA\n",
    "pca =  # ** your code here** \n",
    "  # ** your code here** \n",
    "\n",
    "# projection plane coefficient\n",
    "U=pca.components_ # eigenvectors matrix\n",
    "a=U[0,1]*U[1,2]-U[0,2]*U[1,1]\n",
    "b=U[0,2]*U[1,0]-U[0,0]*U[1,2]\n",
    "c=U[0,0]*U[1,1]-U[0,1]*U[1,0]\n",
    "\n",
    "print(\"The principal eigenvectors are:\\n\",U.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 4: **</font> \n",
    "- Use [transform](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.transform) function of the \"pca\" object to project \"X_norm\" data and put the result in the variable \"Z\".\n",
    "- Use [inverse_transform](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.inverse_transform) function of the \"pca\" object to recover data \"X_rec\" from the projected data \"Z\".\n",
    "- Calculate the projection error: $\\frac{1}{m}\\sum_{i=1}^m\\left \\| x_{norm}^{(i)}-x_{rec}^{(i)} \\right \\|^2$  \n",
    "where: $\\left \\| x \\right \\|^2=\\sum_{j=1}^n x_j^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project data\n",
    "Z =  # ** your code here** \n",
    "\n",
    "# recover data\n",
    "X_rec =  # ** your code here** \n",
    "\n",
    "# calculate error \n",
    "proj_error =  # ** your code here** \n",
    "print(\"The projection error is:\",proj_error)\n",
    "\n",
    "# meshgrid for plane\n",
    "x=np.array([[-0.3,-0.3],[0.3,0.3]])\n",
    "y=np.array([[-0.3,0.3],[-0.3,0.3]])\n",
    "z=-a/c*x-b/c*y\n",
    "\n",
    "# plot data\n",
    "fig = plt.figure(\"PCA 3D data\",figsize=(9,4.5))\n",
    "ax = fig.add_subplot(1, 2, 1,projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2], c=\"b\", marker=\"o\",facecolors='none')\n",
    "ax.scatter(std[0]*X_rec[:,0]+mu[0], std[1]*X_rec[:,1]+mu[1], std[2]*X_rec[:,2]+mu[2], c=\"g\", marker=\"o\",facecolors='none',alpha=0.5)\n",
    "ax.plot_surface(x,y,z,color=\"g\",linewidth=1,alpha=0.2)\n",
    "ax.quiver(0,0,0,a,b,c,color='g', arrow_length_ratio=0.4,length=0.3)\n",
    "for i in range(m):\n",
    "    ax.plot([X[i,0],std[0]*X_rec[i,0]+mu[0]],[X[i,1],std[1]*X_rec[i,1]+mu[1]],[X[i,2],std[2]*X_rec[i,2]+mu[2]],'r--',linewidth=1)\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_zlabel('x_3')\n",
    "ax.set_title(\"Principal component projection\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plt.scatter(Z[:,0],Z[:,1],c=\"g\",alpha=0.5)\n",
    "ax.set_xlabel('z_1')\n",
    "ax.set_ylabel('z_2')\n",
    "ax.set_title(\"Projection plane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faces dataset compression\n",
    "In this last part, we will try to compress face images using principal component analysis (PCA). Then, we will evaluate the compression performance by calculating the compression ratio and the retained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Display_faces(X_display,fig_name,Vmax=None,Vmin=None):\n",
    "    \"\"\"\n",
    "    display face image contained in X_display \n",
    "    \"\"\"\n",
    "    if (Vmax==None):\n",
    "        Vmax=X_display.max()\n",
    "    if (Vmin==None):\n",
    "        Vmin=X_display.min()\n",
    "    # calculate number and size of patches fo each digit image\n",
    "    nbr_select=X_display.shape[0] # number of digits to display\n",
    "    origin_img_size=int(np.sqrt(X_display.shape[1]))\n",
    "    nbr_patch_horiz=int(np.floor(np.sqrt(nbr_select)))\n",
    "    nbr_patch_vertic=int(np.ceil(nbr_select/nbr_patch_horiz))\n",
    "    img_width=32\n",
    "    img_height=32\n",
    "    \n",
    "    # create Display_matrix that contains all image patches\n",
    "    Display_matrix=-np.ones((nbr_patch_vertic*(img_height+1)-1,nbr_patch_horiz*(img_width+1)-1))\n",
    "    for i in range(nbr_patch_vertic):\n",
    "        for j in range(nbr_patch_horiz):\n",
    "            if (i*nbr_patch_horiz+j>=nbr_select):\n",
    "                break\n",
    "            Display_matrix[i*(img_height+1):(i+1)*img_height+i,j*(img_width+1):(j+1)*img_width+j]=np.reshape(X_display[i*nbr_patch_horiz+j,np.arange(origin_img_size**2)%origin_img_size<img_height][:img_width*img_height],(img_height,img_width),order='F')\n",
    "    plt.figure(fig_name,figsize=(max(3,nbr_patch_horiz),max(3,nbr_patch_vertic)))\n",
    "    plt.imshow(Display_matrix,cmap=\"gray\",vmax=Vmax,vmin=Vmin)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 5: **</font> \n",
    "- Load the data from \"face.txt\" file in \"face\" variable and check its size. (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library)  \n",
    "This dataset contains 5000 of $32 \\times 32$ pixel images. These images are flatten and saved on a single line of 1024 pixels on our dataset.\n",
    "- Calculate the covariance matrix \"SIGMA\" given by: $\\Sigma=\\frac{1}{m}X_{norm}^\\top X_{norm}$.\n",
    "- Use [svd](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) function from sklearn library to make singular value decomposition. This function will compute 3 arrays: U, S and V. The arrays \"U\" and \"V\" contain the eigenvectors. They are orthogonal matrix so the inverse is equal to the transpose and they are the inverses of each other. While \"S\" contains the eigenvalues. these arrays verify the following formulas:$$\\Sigma=U\\times diag(S)\\times V$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "face =  # ** your code here** \n",
    "print(\"The size of face dataset is:\",face.shape)\n",
    "m =face.shape[0]\n",
    "\n",
    "# visualize data\n",
    "nbr_select=100\n",
    "np.random.seed(0)\n",
    "rand_perm=np.random.permutation(m)\n",
    "Display_faces(face[rand_perm[:nbr_select],:],'Visualize some images', 128,-128)\n",
    "\n",
    "K = 100 # reduction dimension\n",
    "\n",
    "# Singular value decomposition\n",
    "SIGMA =  # ** your code here** \n",
    "U,S,V =  # ** your code here** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"blue\">**Question 6: **</font> \n",
    "- Calculate the projected data \"Z\" given by: $Z=face\\times U_K$  \n",
    "Where: $U_K$ represent the first K columns of $U$ matrix.\n",
    "- Calculate the recovered data given by: $face_{rec}=Z\\times U_K^{-1}=Z\\times U_K^\\top$ \n",
    "- Calculate compression ratio \"comp_ratio\": number of pixel of original dataset/ (number of pixel of compressed  data+size of K columns of matrix U necessary for recovery)\n",
    "- Calculate the retained variance ratio \"retain_var\": $$1-\\frac{\\sum_{i=1}^m\\left \\| face^{(i)}-face_{rec}^{(i)} \\right \\|^2}{\\sum_{i=1}^m\\left \\| face^{(i)} \\right \\|^2}$$\n",
    "- Change value of \"K\" in the previous block and re-execute to see how retained variance vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data projection and recovery\n",
    "Z =   # ** your code here** \n",
    "face_rec =   # ** your code here** \n",
    "print(Z.shape)\n",
    "#calulate comression ratio and retained varaince\n",
    "comp_ratio =  # ** your code here** \n",
    "print(\"The compression ratio is: %.2f\"%comp_ratio)\n",
    "retain_var =    # ** your code here** \n",
    "print(\"%.2f\"%(retain_var*100),\"% of variance is retained\")\n",
    "\n",
    "# display compressed image and some principal component\n",
    "Display_faces(face_rec[rand_perm[:nbr_select],:],'Compressed images')\n",
    "Display_faces(U[:,:36].T,'Principal components')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
