{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Model Selection, Cross Validation and Regularization\n",
    "\n",
    "In  this session, we will use some already seen datasets to illustrate important techniques like validation and regularization. First, we come back to the student \"exams\" dataset to understand the need of testing data in addition to the training data.  \n",
    "\n",
    "Then, we will illustrate the under-fitting and over-fitting phenomenas on randomly generated data. After that, we will select a model that fits the best our cross validation data.  \n",
    "\n",
    "Finally, we will implement regularization technique on the \"Microchip\" testing dataset. Thus, we could understand how this technique helps to prevent over-fitting. \n",
    "\n",
    "### Train and Test data: Student \"exams\" dataset\n",
    "In this section, we will train a logistic classifier on the training data of student \"exams\" dataset. Then, we will predict the student admission of the test data and compare the accuracy of the classifier on the training and test data.\n",
    "\n",
    "<font color=\"blue\">**Question 1: **</font>The *\"exams_train_data.txt\"* file contains 3 columns that represent the exam 1, exam 2 scores and the results of 100 students (0: Not admitted, 1: Admitted). \n",
    "- Load train data from \"exams_train_data.txt\" file in \"students_results_train\" variable and check its size. (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library)\n",
    "- Implement the \"Poly_Features\" function that concatenates to data array the different possible powers (below deg) and interaction terms of feature vectors f1 and f2 as shown below:$$data=[data,~f_1,~ f_2,~ f_1^2,~ f_1\\times f_2,~ f_2^2,~ \\dots,~ f_1^{deg},~ f_1^{deg-1}\\times f_2,~\\dots,~ f_2^{deg}]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fmin_bfgs\n",
    "\n",
    "#load training data\n",
    "students_results_train = # ** your code here** \n",
    "\n",
    "# you could verify the size of the data using shape() function on numpy array house_data\n",
    "print(\"The training data contains {0} student results. There are {1} columns for each exam score and 1 column for admission\".format(students_results_train.shape[0],students_results_train.shape[1]-1))\n",
    "\n",
    "def Poly_Features(data,f1,f2,deg):\n",
    "    # ** your code here**\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "m_train = students_results_train.shape[0] # number of student\n",
    "x_1_train = students_results_train[:,0,np.newaxis] # we add np.newaxis in the indexing to obtain an array \n",
    "x_2_train = students_results_train[:,1,np.newaxis] # with shape (100,1) instead of (100,)\n",
    "y_train = students_results_train[:,2,np.newaxis] # the student admission result vector\n",
    "\n",
    "# add polynomial features to the array data X\n",
    "degree=2  # degree of polynomial feature\n",
    "X_train=np.ones((m_train,1))   # initialize X array\n",
    "X_train = Poly_Features(X_train,x_1_train,x_2_train,degree)  \n",
    "n = X_train.shape[1]  # number of features\n",
    "print(\"The number of features is: \",n)\n",
    "\n",
    "# define sigmoid function for logistic regression hypothesis \n",
    "def sigmoid(z):\n",
    "    return np.ones(z.shape)/(1+np.exp(-z))\n",
    "\n",
    "# define logistic cost function (inspired from max likelihood)\n",
    "def cost_func(theta):\n",
    "    J=np.sum(-y_train*np.log(sigmoid(np.dot(X_train,theta[:,np.newaxis]))))-np.sum((1-y_train)*np.log(1-sigmoid(np.dot(X_train,theta[:,np.newaxis]))))\n",
    "    return J/m_train  \n",
    "\n",
    "# define the gradient of logistic cost function \n",
    "def grad_cost_func(theta):\n",
    "    g=(1/m_train)*(np.dot(X_train.transpose(),(sigmoid(np.dot(X_train,theta[:,np.newaxis]))-y_train)))  # this is the vectorized implementation\n",
    "    g.shape=(g.shape[0],)\n",
    "    return g  \n",
    "\n",
    "# calculate the optimal theta\n",
    "theta0=np.zeros((n,),dtype=float)\n",
    "theta_opt= fmin_bfgs(cost_func,theta0,fprime=grad_cost_func,disp=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 2: **</font>\n",
    "- Predict \"y_pred_train\" the admission result of each student on train data.\n",
    "- Calculate the training accuracy (number of good prediction/number of all student) on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# calculate predection and train accuracy\n",
    "#y_pred = sigmoid(np.dot(X,theta_opt))>=0.5\n",
    "y_pred_train =    # ** your code here** \n",
    "train_accuracy =  # ** your code here** \n",
    "print(\"The accuracy on the training data is:\", train_accuracy,\"%\")\n",
    "\n",
    "# calculate the mesh grid for contour plot\n",
    "u1=np.linspace(5,20,100)\n",
    "u2=np.linspace(5,20,100)\n",
    "u1, u2 = np.meshgrid(u1, u2)\n",
    "\n",
    "X3=np.ones((*u1.shape,1))\n",
    "for i in range(1,degree+1):\n",
    "    for j in range(i+1):\n",
    "         X3 = np.concatenate((X3,u1[...,np.newaxis]**(i-j)*u2[...,np.newaxis]**j),axis=-1)\n",
    "\n",
    "Z=np.dot(X3,theta_opt)\n",
    "\n",
    "# plot descision boundries\n",
    "plt.figure(\"Admission decision boundries\",figsize=(9,5))\n",
    "fail=plt.scatter(x_1_train[y_train==0], x_2_train[y_train==0],  color='red',label='fail')\n",
    "success=plt.scatter(x_1_train[y_train==1], x_2_train[y_train==1],  color='green',marker='+',s=80,label='success')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.title('Adimitted/Not admitted Students')\n",
    "ctr = plt.contour(u1, u2, Z,0,colors=\"blue\")\n",
    "extra = Rectangle((0, 0), 3, 4, fc=\"w\", fill=False, edgecolor=\"b\", linewidth=1)\n",
    "plt.legend([extra,fail,success], (\"decision boundries\",\"fail\",\"success\"),loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 3: **</font>\n",
    "- Load test data from \"exams_test_data.txt\" file in \"students_results_test\" variable and check its size. (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library).\n",
    "- Calculate the test accuracy (number of good prediction/number of all student) on the test data and compare it with the train accuracy. Interpret the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "students_results_test =  # ** your code here** \n",
    "\n",
    "m_test = students_results_test.shape[0] # number of student in test data\n",
    "x_1_test = students_results_test[:,0,np.newaxis] # we add np.newaxis in the indexing to obtain an array \n",
    "x_2_test = students_results_test[:,1,np.newaxis] # with shape (100,1) instead of (100,)\n",
    "y_test = students_results_test[:,2,np.newaxis] # we add np.newaxis in the indexing to obtain an array with shape (100,1) instead of (100,)\n",
    "\n",
    "# add polynomial features to the array data X\n",
    "X_test=np.ones((m_test,1))   # initialize X array\n",
    "X_test = Poly_Features(X_test,x_1_test,x_2_test,degree)\n",
    "\n",
    "# calculate prediction and accuracy on test data\n",
    "y_test_pred =  # ** your code here** \n",
    "test_accuracy = # ** your code here** \n",
    "print(\"The accuracy on the test data is:\", test_accuracy,\"%\")\n",
    "\n",
    "# plot descision boundries and test data\n",
    "plt.figure(\"decision boundries and test data\",figsize=(9,5))\n",
    "fail=plt.scatter(x_1_test[y_test==0], x_2_test[y_test==0],  color='red')\n",
    "success=plt.scatter(x_1_test[y_test==1], x_2_test[y_test==1],  color='green',marker='+',s=80)\n",
    "fail_train=plt.scatter(x_1_train[y_train==0], x_2_train[y_train==0],  color='gray')\n",
    "success_train=plt.scatter(x_1_train[y_train==1], x_2_train[y_train==1],  color='gray',marker='+',s=80)\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.title('Adimitted/Not admitted Students')\n",
    "ctr = plt.contour(u1, u2, Z,0,colors=\"blue\")\n",
    "plt.legend([extra,fail,success,fail_train,success_train], (\"decision boundries\",\"test data (fail)\",\"test data (success)\",\"train data (fail)\",\"train data (success)\"),loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "In this part we will study the effect of the number of features and the model complexity on the training phase and we will illustrate the underfitting and overfitting phenomena. We will use randomly generated data for a regression problem and we will try to use several models with different number of features (different polynomial features degrees). Then, we will see how well the model fits the training and the test data.\n",
    "\n",
    "<font color=\"blue\">**Question 4: **</font>\n",
    "- Split the data (\"x\" and \"y\" vector) to training and test data with size \"m_train\" and \"m_test\" respectively. When the original data is sorted, you should choose the training and the test sets randomly to ensure that the two sets cover the possible values space in a best manner.  \n",
    "**Hint:** You could use [random permutation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html) function to generate a random permutation of \"m\" indices for \"x\" and \"y\" vectors. Then, you could select the first \"m_train\" indices to index train data from \"x\" and \"y\". You could use the rest of indices to index the test data.\n",
    "- Use [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class and [PolynomialFeatures.fit_transform](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures.fit_transform) functions from numpy library to generate polynomial features of degree \"i\". This will generate automatically the features instead of adding them by hand as in the previous example with the implemented function \"Poly_Features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# generate random data\n",
    "np.random.seed(0)\n",
    "m = 15\n",
    "x = np.linspace(0,10,m) + np.random.randn(m)/5\n",
    "y = np.sin(x)+x/6 + np.random.randn(m)/10\n",
    "\n",
    "# calculate the size of training and test sets\n",
    "train_ratio = 0.75\n",
    "m_train = int(round(train_ratio*m)) \n",
    "m_test = m-m_train\n",
    "print(\"the size of training set is:\",m_train,\"\\nthe size of test set is:\",m_test)\n",
    "\n",
    "# split the data to training and test sets\n",
    "np.random.seed(9599)\n",
    "rand_perm =  # ** your code here**\n",
    "X_train = # ** your code here**\n",
    "y_train = # ** your code here**\n",
    "X_test =  # ** your code here** \n",
    "y_test =  # ** your code here**\n",
    "\n",
    "# visualize training and test set\n",
    "plt.figure(\"training and test set\")\n",
    "plt.scatter(X_train, y_train, label='training data')\n",
    "plt.scatter(X_test, y_test, label='test data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# train several polynomial models\n",
    "regr = LinearRegression()\n",
    "deg=[1,3,6,9,11]\n",
    "Poly_predict=np.zeros((len(deg),200))\n",
    "for i in range(len(deg)):\n",
    "    poly =  # ** your code here** \n",
    "    new_X =  # ** your code here** \n",
    "    regr.fit(new_X, y_train[:,np.newaxis])\n",
    "    u = np.linspace(0,11.5,200)   # \n",
    "    new_U = poly.fit_transform(u[:,np.newaxis])\n",
    "    Poly_predict[i,:]=regr.predict(new_U).transpose()\n",
    "\n",
    "# visualize different polynomial models\n",
    "plt.figure(\"different polynomial models\",figsize=(9,5))\n",
    "plt.plot(X_train, y_train, 'o', label='training data', markersize=10)\n",
    "plt.plot(X_test, y_test, 'o', label='test data', markersize=10)\n",
    "for i,degree in enumerate(deg):\n",
    "    plt.plot(u, Poly_predict[i], alpha=0.8, lw=2, label='degree={}'.format(degree))\n",
    "plt.ylim(-1.5,4)\n",
    "plt.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">**Notes: **</font>  \n",
    "\n",
    "We note that the different polynomial models fit our data differently. For instance, the polynomial model with degree 1 (linear model) fits poorly our data. It doesn't explain a lot of variation in the data since it doesn't include enough features. We say that this is an **\"underfitting\"** or a **\"high bias\"** problem.  \n",
    "\n",
    "On the other hand, when using higher order polynomial with degree 11, we note that the model go through all the points in training data. However, it is not a good fit for our data because it introduce a lot of variation and a lot of features and it doesn't generalize well for test data. This is called **\"overfitting\"** or **\"high variance\"** problem.  \n",
    "\n",
    "In order to choose the best polynomial order that fits our data, we will have recourse to model selection techniques.\n",
    "\n",
    "### Model Selection and Cross Validation\n",
    "In this part, we will train several polynomial models. Then, we will assess their performance on the training and the test sets. Hence, we will choose  the model with best performance in both training and test set. However, the calculated performance on test data won't be a good estimation of the performance of our model in general case. In fact, our model order (polynomial degree) is fitted to the test data. Thus, it tends to perform better on test data than on a new data. Therefore, we introduce the cross validation data used for tuning model meta-parameters (polynomial degree, classification threshold...). Then, we will use the test data to estimate the performance of our model in general case.  \n",
    "\n",
    "<font color=\"blue\">**Question 5: **</font>\n",
    "- Split the original data (\"x\" and \"y\" vector) to training, validation and test data with size \"m_train\", \"m_val\" and \"m_test\" respectively.  \n",
    "**Hint:** You could use [random permutation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html) function to generate a random permutation of \"m\" indices for \"x\" and \"y\" vectors. Then, you could select the first \"m_train\" indices to index train data from \"x\" and \"y\". Then, you can act similarly for validation and test sets.\n",
    "- Calculate \"train_error\" and \"val_error\" mean squared error on training and validation set for each polynomial model with degree \"i\" (for loop counter).  \n",
    "**Hint:** You could use [mean_squared_error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function from sklearn library to evaluate the mean squared error between original \"y\" and \"y_predicted\".\n",
    "- From the train and validation error graph and values, select the polynomial degree \"best_poly_deg\" that fit the best our data. Then, compare the training, validation and test error of this polynomial model. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.regression import mean_squared_error\n",
    "\n",
    "# calculate the size of training, validation and test sets\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "m_train = int(round(train_ratio*m)) \n",
    "m_val = int(round(val_ratio*m)) \n",
    "m_test = m-m_train-m_val\n",
    "print(\"the size of training set is:\",m_train,\"\\nthe size of validation set is:\",m_val,\"\\nthe size of test set is:\",m_test)\n",
    "\n",
    "# split data to training, validation and test sets\n",
    "np.random.seed(5190969)\n",
    "rand_perm = # ** your code here**\n",
    "X_train = # ** your code here**\n",
    "y_train = # ** your code here**\n",
    "X_val = # ** your code here**\n",
    "y_val = # ** your code here**\n",
    "X_test = # ** your code here**\n",
    "y_test = # ** your code here**\n",
    "\n",
    "# visualize training, validation and test sets\n",
    "plt.figure(\"training, validation and test sets\")\n",
    "plt.scatter(X_train, y_train, label='training data')\n",
    "plt.scatter(X_val, y_val, label='validation data')\n",
    "plt.scatter(X_test, y_test, label='test data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# train and assess several polynomial models\n",
    "train_error = np.zeros((deg[-1]+1,)) \n",
    "val_error = np.zeros((deg[-1]+1,))\n",
    "\n",
    "for i in range(deg[-1]+1):\n",
    "    poly = PolynomialFeatures(i)\n",
    "    new_X_train= poly.fit_transform(X_train[:,np.newaxis])\n",
    "    new_X_val = poly.fit_transform(X_val[:,np.newaxis])\n",
    "    regr.fit(new_X_train, y_train[:,np.newaxis])\n",
    "    y_train_predicted = regr.predict(new_X_train)\n",
    "    y_val_predicted = regr.predict(new_X_val)\n",
    "    \n",
    "    train_error[i] =  # ** your code here**\n",
    "    val_error[i] =    # ** your code here**\n",
    "    print(\"degree {} polynomial has a train error: {:.5f} and validation error: {:.4f}\".format(i,train_error[i],val_error[i]))\n",
    "\n",
    "# visualize error of each polynomial model\n",
    "plt.figure(\"train and validation error of each polynomial model\")  \n",
    "plt.plot(range(deg[-1]+1),train_error[:deg[-1]+1],label=\"train error\")\n",
    "plt.plot(range(deg[-1]+1),val_error[:deg[-1]+1],label=\"validation error\")\n",
    "plt.ylim(0,1)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "best_poly_deg = # ** your code here**\n",
    "poly = PolynomialFeatures(best_poly_deg)\n",
    "new_X_train= poly.fit_transform(X_train[:,np.newaxis])\n",
    "new_X_test = poly.fit_transform(X_test[:,np.newaxis])\n",
    "regr.fit(new_X_train, y_train[:,np.newaxis])\n",
    "y_test_predicted = regr.predict(new_X_test)\n",
    "test_error = mean_squared_error(y_test, y_test_predicted)\n",
    "print(\"train error =\",train_error[best_poly_deg],\"\\nvalidation error =\",val_error[best_poly_deg],\"\\ntest error =\",test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "In this section, we will split the \"Microchip\" dataset into training and test sets. We will train a polynomial logistic classifier with and without regularization. Then, we will compare train and test accuracy in the two cases.  \n",
    "\n",
    "The regularization helps to avoid the problem of overfitting by reducing or even making null $\\theta_j's$ of non significant features. The idea is to include the sum of $\\theta_j's$ in cost function in order to reduce them and remove useless features in our models.\n",
    "\n",
    "The regularized cost function is equal to: $$J_{Reg}(\\theta)= J(\\theta)+\\frac{\\lambda}{2m}\\times\\sum_{j=1}^{n-1} \\theta_j^2$$\n",
    "\n",
    "The gradient of regularized cost function is equal to: \n",
    "\n",
    "$$\\nabla J_{Reg}(\\theta) = \\begin{bmatrix}\\frac{\\partial J(\\theta)}{\\partial \\theta_0}\n",
    "\\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_1}\n",
    "\\\\ \\vdots\n",
    "\\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n-1}}\n",
    "\\end{bmatrix}+\\frac{\\lambda}{m} \\begin{bmatrix}0\n",
    "\\\\ \\theta_1\n",
    "\\\\ \\vdots\n",
    "\\\\ \\theta_{n-1}\n",
    "\\end{bmatrix}$$ \n",
    "where: $ \\left\\{\\begin{matrix}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m}{(h_\\theta(x_i) - y)~x_j} ~~for~ j=0\\dots n-1 \n",
    "\\\\ h_\\theta(x_i)=sigmoid(\\theta^\\top x_i)=\\frac{1}{1+e^{-\\theta^\\top x_i}}\n",
    "\\end{matrix}\\right.$\n",
    "<font color=\"blue\">**Question 6: **</font>\n",
    "- Load data from \"microchip.txt\" file (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library).\n",
    "- Split \"X\" and \"Y\" data into train and test sets with a size of 90% and 10% respectively.  \n",
    "**Hint: ** You could use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn library instead of generating random permutation by hand.\n",
    "- Implement regularized cost and gradient function according to equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load and extract data\n",
    "microchip_data =  # ** your code here** \n",
    "m = microchip_data.shape[0] # number of mocrochips\n",
    "x_1 = microchip_data[:,0,np.newaxis] # we add np.newaxis in the indexing to obtain an array \n",
    "x_2 = microchip_data[:,1,np.newaxis] # with shape (118,1) instead of (118,)\n",
    "Y = microchip_data[:,2,np.newaxis] # we add np.newaxis in the indexing to obtain an array with shape (118,1) instead of (118,)\n",
    "\n",
    "# split data to train and test sets\n",
    "X_train, X_test, y_train, y_test = # ** your code here** \n",
    "\n",
    "def Reg_cost_func(theta):\n",
    "    # ** your code here** \n",
    "    \n",
    "    return J\n",
    "\n",
    "def Reg_grad_cost_func(theta):\n",
    "    # ** your code here** \n",
    "    \n",
    "    return g  \n",
    "\n",
    "# model meta-parameters\n",
    "degree=6       # degree of polynomial features\n",
    "lambda_=0      # regularization coefficient \n",
    "print(\"The degree of the polynomial model is:\",degree,\"\\nThe regularization coefficient \\u03BB is:\",lambda_,\" (no regularization)\" if (lambda_==0) else \"\")  #\\u03BB is the unicode caractère lambda\n",
    "\n",
    "# add polynomial features to the train data\n",
    "x_1_train = X_train[:,0,np.newaxis] \n",
    "x_2_train = X_train[:,1,np.newaxis]\n",
    "y=y_train\n",
    "\n",
    "X=np.ones((X_train.shape[0],1))   # initialize X array\n",
    "X = Poly_Features(X,x_1_train,x_2_train,degree)\n",
    "n = X.shape[1]\n",
    "\n",
    "# calculate optimal theta\n",
    "theta0=np.zeros((n,))\n",
    "theta_opt = fmin_bfgs(Reg_cost_func,theta0,fprime=Reg_grad_cost_func,disp=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 7: **</font>\n",
    "- Calculate the train and test accuracies. \n",
    "- In the previous block of code, vary the values of \"degree\" (between 2-8) and the values of lambda (between 0: no regularization to 100: a lot of regularization. You could try also 0.1, 1, 10... values). What is the effect of regularization and lambda parameters?\n",
    "- Compare the value of train and test accuracy in the case with regularization (lambda=0.1 or 1) and without (lambda=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate train accuracy\n",
    "y_train_pred =    # ** your code here** \n",
    "train_accuracy =  # ** your code here** \n",
    "print(\"The accuracy on the training data is:\", train_accuracy,\"%\")\n",
    "\n",
    "# add polynomial features to the test data  \n",
    "x_1_test = X_test[:,0,np.newaxis] \n",
    "x_2_test = X_test[:,1,np.newaxis]\n",
    "X_test_poly=np.ones((X_test.shape[0],1))   # initialize X_test_poly array\n",
    "X_test_poly = Poly_Features(X_test_poly,x_1_test,x_2_test,degree)\n",
    "\n",
    "# calculate test accuracy\n",
    "y_test_pred =   # ** your code here** \n",
    "test_accuracy = # ** your code here** \n",
    "print(\"The accuracy on the test data is:\", test_accuracy,\"%\")\n",
    "\n",
    "# calculate the mesh grid for contour plot\n",
    "u1=np.linspace(-1,1.5,50)\n",
    "u2=np.linspace(-1,1.5,50)\n",
    "u1, u2 = np.meshgrid(u1, u2)\n",
    "X3=np.ones((*u1.shape,1))\n",
    "for i in range(1,degree+1):\n",
    "    for j in range(i+1):\n",
    "         X3 = np.concatenate((X3,u1[...,np.newaxis]**(i-j)*u2[...,np.newaxis]**j),axis=-1)\n",
    "Z=np.dot(X3,theta_opt)\n",
    "\n",
    "# plot descision boundries\n",
    "plt.figure(\"Microchip decision boundries\",figsize=(9,5))\n",
    "fail=plt.scatter(x_1_test[y_test==0], x_2_test[y_test==0],  color='red',label='fail')\n",
    "success=plt.scatter(x_1_test[y_test==1], x_2_test[y_test==1],  color='green',marker='+',s=80,label='success')\n",
    "fail_train=plt.scatter(x_1_train[y_train==0], x_2_train[y_train==0],  color='gray',label='fail')\n",
    "success_train=plt.scatter(x_1_train[y_train==1], x_2_train[y_train==1],  color='gray',marker='+',s=80,label='success')\n",
    "fail_train=plt.scatter(x_1_train[y_train==0], x_2_train[y_train==0],  color='gray',label='fail')\n",
    "success_train=plt.scatter(x_1_train[y_train==1], x_2_train[y_train==1],  color='gray',marker='+',s=80,label='success')\n",
    "plt.xlabel('Test 1 score')\n",
    "plt.ylabel('Test 2 score')\n",
    "plt.title('Accepted/Rejected Microchip')\n",
    "ctr = plt.contour(u1, u2, Z,0,colors=\"blue\")\n",
    "plt.legend([extra,fail,success,fail_train,success_train], (\"decision boundries\",\"test data (fail)\",\"test data (success)\",\"train data (fail)\",\"train data (success)\"),loc='best')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
